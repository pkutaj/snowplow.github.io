"use strict";(self.webpackChunkdocsite_poc_github_io=self.webpackChunkdocsite_poc_github_io||[]).push([[99905],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>m});var r=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},s=Object.keys(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=r.createContext({}),u=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},p=function(e){var t=u(e.components);return r.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,s=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),d=u(n),m=o,h=d["".concat(l,".").concat(m)]||d[m]||c[m]||s;return n?r.createElement(h,a(a({ref:t},p),{},{components:n})):r.createElement(h,a({ref:t},p))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var s=n.length,a=new Array(s);a[0]=d;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:o,a[1]=i;for(var u=2;u<s;u++)a[u]=n[u];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},94124:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>c,frontMatter:()=>s,metadata:()=>i,toc:()=>u});var r=n(87462),o=(n(67294),n(3905));const s={title:"Run EmrEtlRunner",date:"2020-02-26",sidebar_position:30},a=void 0,i={unversionedId:"getting-started-on-snowplow-open-source/setup-snowplow-on-aws/setup-destinations/setup-redshift/setup-emretlrunner/run-emretlrunner/index",id:"getting-started-on-snowplow-open-source/setup-snowplow-on-aws/setup-destinations/setup-redshift/setup-emretlrunner/run-emretlrunner/index",title:"Run EmrEtlRunner",description:"Commands",source:"@site/docs/getting-started-on-snowplow-open-source/setup-snowplow-on-aws/setup-destinations/setup-redshift/setup-emretlrunner/run-emretlrunner/index.md",sourceDirName:"getting-started-on-snowplow-open-source/setup-snowplow-on-aws/setup-destinations/setup-redshift/setup-emretlrunner/run-emretlrunner",slug:"/getting-started-on-snowplow-open-source/setup-snowplow-on-aws/setup-destinations/setup-redshift/setup-emretlrunner/run-emretlrunner/",permalink:"/docs/getting-started-on-snowplow-open-source/setup-snowplow-on-aws/setup-destinations/setup-redshift/setup-emretlrunner/run-emretlrunner/",draft:!1,editUrl:"https://github.com/snowplow/snowplow.github.io/tree/main/docs/getting-started-on-snowplow-open-source/setup-snowplow-on-aws/setup-destinations/setup-redshift/setup-emretlrunner/run-emretlrunner/index.md",tags:[],version:"current",lastUpdatedAt:1661290387,formattedLastUpdatedAt:"Aug 23, 2022",sidebarPosition:30,frontMatter:{title:"Run EmrEtlRunner",date:"2020-02-26",sidebar_position:30},sidebar:"tutorialSidebar",previous:{title:"Configure EmrEtlRunner",permalink:"/docs/getting-started-on-snowplow-open-source/setup-snowplow-on-aws/setup-destinations/setup-redshift/setup-emretlrunner/configuring-emretlrunner/"},next:{title:"Schedule EmrEtlRunner",permalink:"/docs/getting-started-on-snowplow-open-source/setup-snowplow-on-aws/setup-destinations/setup-redshift/setup-emretlrunner/schedule-emretlrunner/"}},l={},u=[{value:"Commands",id:"commands",level:2},{value:"Run command",id:"run-command",level:3},{value:"2.2 Lint commands",id:"22-lint-commands",level:3},{value:"Checking the results",id:"checking-the-results",level:2},{value:"Next steps",id:"next-steps",level:2}],p={toc:u};function c(e){let{components:t,...n}=e;return(0,o.kt)("wrapper",(0,r.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"commands"},"Commands"),(0,o.kt)("h3",{id:"run-command"},"Run command"),(0,o.kt)("p",null,"The most useful command is the ",(0,o.kt)("inlineCode",{parentName:"p"},"run")," command which allows you to actually run your EMR job:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"$ ./snowplow-emr-etl-runner run\n")),(0,o.kt)("p",null,"The available options are as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Usage: run [options]\n    -c, --config CONFIG              configuration file\n    -n, --enrichments ENRICHMENTS    enrichments directory\n    -r, --resolver RESOLVER          Iglu resolver file\n    -t, --targets TARGETS            targets directory\n    -d, --debug                      enable EMR Job Flow debugging\n    -f {enrich,shred,elasticsearch,archive_raw,rdb_load,analyze,archive_enriched,archive_shredded,staging_stream_enrich},\n        --resume-from                resume from the specified step\n    -x {staging,enrich,shred,elasticsearch,archive_raw,rdb_load,consistency_check,analyze,load_manifest_check,archive_enriched,archive_shredded,staging_stream_enrich},\n        --skip                       skip the specified step(s)\n    -i, --include {vacuum}           include additional step(s)\n    -l, --lock PATH                  where to store the lock\n        --ignore-lock-on-start       ignore the lock if it is set when starting\n        --consul ADDRESS             address to the Consul server\n")),(0,o.kt)("p",null,"Note that the ",(0,o.kt)("inlineCode",{parentName:"p"},"config")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"resolver")," options are mandatory."),(0,o.kt)("p",null,"Note that in ",(0,o.kt)("em",{parentName:"p"},"Stream Enrich mode")," you cannot skip nor resume from ",(0,o.kt)("inlineCode",{parentName:"p"},"staging"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"enrich")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"archive_raw"),". Instead of ",(0,o.kt)("inlineCode",{parentName:"p"},"staging")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"enrich"),", in Stream Enrich mode single special ",(0,o.kt)("inlineCode",{parentName:"p"},"staging_stream_enrich")," is used."),(0,o.kt)("h3",{id:"22-lint-commands"},"2.2 Lint commands"),(0,o.kt)("p",null,"Other useful commands include the ",(0,o.kt)("inlineCode",{parentName:"p"},"lint")," commands which allows you to check the validity of your resolver or enrichments with respect to their respective schemas."),(0,o.kt)("p",null,"If you want to lint your resolver:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"$ ./snowplow-emr-etl-runner lint resolver\n")),(0,o.kt)("p",null,"The mandatory options are:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Usage: lint resolver [options]\n    -r, --resolver RESOLVER          Iglu resolver file\n")),(0,o.kt)("p",null,"If you want to lint your enrichments:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"$ ./snowplow-emr-etl-runner lint enrichments\n")),(0,o.kt)("p",null,"The mandatory options are:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Usage: lint enrichments [options]\n    -r, --resolver RESOLVER          Iglu resolver file\n    -n, --enrichments ENRICHMENTS    enrichments directory\n")),(0,o.kt)("h2",{id:"checking-the-results"},"Checking the results"),(0,o.kt)("p",null,"Once you have run the EmrEtlRunner you should be able to manually inspect in S3 the folder specified in the ",(0,o.kt)("inlineCode",{parentName:"p"},":out:")," parameter in your ",(0,o.kt)("inlineCode",{parentName:"p"},"config.yml")," file and see new files generated, which will contain the cleaned data either for uploading into a storage target (e.g. Redshift) or for analysing directly using Hive or Spark or some other querying tool on EMR."),(0,o.kt)("p",null,"Note: most Snowplow users run the 'spark' version of the ETL process, in which case the data generated is saved into subfolders with names of the form ",(0,o.kt)("inlineCode",{parentName:"p"},"part-000..."),". If, however, you are running the legacy 'hive' ETL (because e.g. you want to use Hive as your storage target, rather than Redshift, which is the only storage target the 'spark' etl currently supports), the subfolders names will be of the format ",(0,o.kt)("inlineCode",{parentName:"p"},"dt=..."),"."),(0,o.kt)("h2",{id:"next-steps"},"Next steps"),(0,o.kt)("p",null,"Comfortable using EmrEtlRunner? Then ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/snowplow/wiki/3-Scheduling-EmrEtlRunner"},"schedule it")," so that it regularly takes new data generated by stream enrich, shreds it (using RDB Shredder) and loaders it into Redshift (using RDB Loader)."))}c.isMDXComponent=!0}}]);