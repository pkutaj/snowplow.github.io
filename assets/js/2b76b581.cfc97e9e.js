"use strict";(self.webpackChunkdocsite_poc_github_io=self.webpackChunkdocsite_poc_github_io||[]).push([[37444],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var r=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=r.createContext({}),p=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=p(n),h=o,m=d["".concat(s,".").concat(h)]||d[h]||c[h]||a;return n?r.createElement(m,i(i({ref:t},u),{},{components:n})):r.createElement(m,i({ref:t},u))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,i[1]=l;for(var p=2;p<a;p++)i[p]=n[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},64607:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>a,metadata:()=>l,toc:()=>p});var r=n(87462),o=(n(67294),n(3905));const a={title:"EmrEtlRunner",date:"2020-11-09",sidebar_position:9990},i=void 0,l={unversionedId:"migrated/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/index",id:"migrated/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/index",title:"EmrEtlRunner",description:"Snowplow\xa0EmrEtlRunner\xa0is a deprecated application that ran Snowplow's batch processing jobs in AWS EMR, such as the RDB shredder. See the RDB loader R35 upgrade guide for how to migrate away from this application.",source:"@site/docs/migrated/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/index.md",sourceDirName:"migrated/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner",slug:"/migrated/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/",permalink:"/docsite-poc.github.io/docs/migrated/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/",draft:!1,tags:[],version:"current",lastUpdatedAt:1660568250,formattedLastUpdatedAt:"Aug 15, 2022",sidebarPosition:9990,frontMatter:{title:"EmrEtlRunner",date:"2020-11-09",sidebar_position:9990},sidebar:"tutorialSidebar",previous:{title:"Events Manifest Populator",permalink:"/docsite-poc.github.io/docs/migrated/pipeline-components-and-applications/loaders-storage-targets/events-manifest-populator/"},next:{title:"Common configuration",permalink:"/docsite-poc.github.io/docs/migrated/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/common-configuration/"}},s={},p=[{value:"Setting up EmrEtlRunner",id:"setting-up-emretlrunner",level:2},{value:"Installing EmrEtlRunner",id:"installing-emretlrunner",level:2},{value:"Assumptions",id:"assumptions",level:3},{value:"Dependencies",id:"dependencies",level:3},{value:"Hardware",id:"hardware",level:4},{value:"EC2 key",id:"ec2-key",level:4},{value:"S3 locations",id:"s3-locations",level:4},{value:"Installation",id:"installation",level:3},{value:"Configuration",id:"configuration",level:3},{value:"Storage targets",id:"storage-targets",level:4},{value:"Iglu",id:"iglu",level:4},{value:"Using EmrEtlRunner",id:"using-emretlrunner",level:2},{value:"Commands",id:"commands",level:3},{value:"Run command",id:"run-command",level:4},{value:"Lint commands",id:"lint-commands",level:4},{value:"Checking the results",id:"checking-the-results",level:3},{value:"Scheduling EmrEtlRunner",id:"scheduling-emretlrunner",level:2},{value:"cron",id:"cron",level:3},{value:"Jenkins",id:"jenkins",level:3},{value:"Windows Task Scheduler",id:"windows-task-scheduler",level:3},{value:"Configuring shredding",id:"configuring-shredding",level:2},{value:"Pre-requisites",id:"pre-requisites",level:3},{value:"Configuring EmrEtlRunner for shredding",id:"configuring-emretlrunner-for-shredding",level:3},{value:"Technical explanation",id:"technical-explanation",level:2},{value:"Setting up end-to-end encryption",id:"setting-up-end-to-end-encryption",level:2},{value:"Pre-requisites",id:"pre-requisites-1",level:3},{value:"Encrypting S3 buckets",id:"encrypting-s3-buckets",level:4},{value:"Setting up an EMR security configuration",id:"setting-up-an-emr-security-configuration",level:4},{value:"At rest encryption in S3",id:"at-rest-encryption-in-s3",level:4},{value:"At rest encryption on local disks",id:"at-rest-encryption-on-local-disks",level:4},{value:"In-transit encryption (Spark and MapReduce)",id:"in-transit-encryption-spark-and-mapreduce",level:4},{value:"Configuring EmrEtlRunner for end-to-end encryption",id:"configuring-emretlrunner-for-end-to-end-encryption",level:3},{value:"Setting up EC2 instance for EmrEtlRunner and StorageLoader",id:"setting-up-ec2-instance-for-emretlrunner-and-storageloader",level:2},{value:"Prepare your system",id:"prepare-your-system",level:3},{value:"Setting up EC2 instance for EmrEtlRunner/StorageLoader",id:"setting-up-ec2-instance-for-emretlrunnerstorageloader",level:3},{value:"<strong>Find your Default VPC ID</strong>",id:"find-your-default-vpc-id",level:3},{value:"<strong>Create Security Group for SSH access</strong>",id:"create-security-group-for-ssh-access",level:3},{value:"<strong>Add rule allowing SSH access from anywhere</strong>",id:"add-rule-allowing-ssh-access-from-anywhere",level:3},{value:"<strong>Create SSH key-pair named on the local machine</strong>",id:"create-ssh-key-pair-named-on-the-local-machine",level:3},{value:"<strong>Run t2.small instance with Amazon Linux AMI with previously created SSH-key</strong>",id:"run-t2small-instance-with-amazon-linux-ami-with-previously-created-ssh-key",level:3},{value:"<strong>Attach security group to Instance</strong>",id:"attach-security-group-to-instance",level:3},{value:"<strong>Check public IP-address of newly created Instance</strong>",id:"check-public-ip-address-of-newly-created-instance",level:3},{value:"<strong>Log-in</strong>",id:"log-in",level:3}],u={toc:p};function c(e){let{components:t,...a}=e;return(0,o.kt)("wrapper",(0,r.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Snowplow\xa0EmrEtlRunner\xa0is a ",(0,o.kt)("em",{parentName:"p"},"deprecated")," application that ran Snowplow's batch processing jobs in ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/emr/"},"AWS EMR"),", such as the ",(0,o.kt)("a",{parentName:"p",href:"/docs/migrated/pipeline-components-and-applications/loaders-storage-targets/snowplow-rdb-loader/rdb-shredder/"},"RDB shredder"),". See the ",(0,o.kt)("a",{parentName:"p",href:"/docs/migrated/pipeline-components-and-applications/loaders-storage-targets/snowplow-rdb-loader/r35-upgrade-guide/"},"RDB loader R35 upgrade guide")," for how to migrate away from this application."),(0,o.kt)("p",null,"Historically it was also used for enriching data, but that functionality was deprecated even longer ago in favour of the ",(0,o.kt)("a",{parentName:"p",href:"/docs/migrated/pipeline-components-and-applications/enrichment-components/"},"streaming versions of Enrich"),"."),(0,o.kt)("h2",{id:"setting-up-emretlrunner"},"Setting up EmrEtlRunner"),(0,o.kt)("p",null,"This guide covers:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"#installing"},"Installation"),". You need to install EmrEtlRunner on your own server. It will interact with Amazon Elastic MapReduce and S3 via the Amazon API"),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"#using"},"Usage"),". How to use EmrEtlRunner at the command line, to instruct it to process data from your collector"),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"#scheduling"},"Scheduling"),". How to schedule the tool so that you always have an up to date set of cleaned, enriched data available for analysis"),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"#shredding"},"Configuring shredding"),". How to configure Snowplow to shred custom self-describing events (also called unstructured events) and contexts ready for loading into dedicated tables in Redshift")),(0,o.kt)("p",null,"In this guide you'll also find additional information:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A ",(0,o.kt)("a",{parentName:"li",href:"#technical"},"deeper technical explanation")," of EtlEmrRunner"),(0,o.kt)("li",{parentName:"ul"},"A guide for ",(0,o.kt)("a",{parentName:"li",href:"#encryption"},"setting up end to end encryption")),(0,o.kt)("li",{parentName:"ul"},"A guide to ",(0,o.kt)("a",{parentName:"li",href:"#ec2"},"setting up an EC2 instance")," for EmrEtlRunner")),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"installing-emretlrunner"},"Installing EmrEtlRunner"),(0,o.kt)("h3",{id:"assumptions"},"Assumptions"),(0,o.kt)("p",null,"This guide assumes that you have administrator access to a Unix-based server (e.g. Ubuntu, OS X, Fedora) on which you can install EmrEtlRunner and schedule a regular cronjob."),(0,o.kt)("p",null,"You might wish to try out the\xa0",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/snowplow/wiki/Setting-up-EC2-instance-for-EmrEtlRunner-and-StorageLoader"},"steps"),"\xa0showing you how an EC2 instance could be set up via\xa0",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/cli/"},"AWS CLI"),"."),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"In theory EmrEtlRunner can be deployed onto a Windows-based server, using the Windows Task Scheduler instead of cron, but this has not been tested or documented.")),(0,o.kt)("h3",{id:"dependencies"},"Dependencies"),(0,o.kt)("h4",{id:"hardware"},"Hardware"),(0,o.kt)("p",null,"You will need to setup EmrEtlRunner on your own server. A number of people choose to do so on an EC2 instance (thereby keeping all of Snowplow in the Amazon Cloud). If you do so, please note that you\xa0",(0,o.kt)("strong",{parentName:"p"},"must not use a\xa0",(0,o.kt)("inlineCode",{parentName:"strong"},"t1.micro"),"\xa0instance"),". You should at the very least use an\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"m1.small"),"\xa0instance."),(0,o.kt)("h4",{id:"ec2-key"},"EC2 key"),(0,o.kt)("p",null,"You will also need an\xa0",(0,o.kt)("strong",{parentName:"p"},"EC2 key pair"),"\xa0setup in your Amazon EMR account."),(0,o.kt)("p",null,"For details on how to do this, please see\xa0",(0,o.kt)("a",{parentName:"p",href:"http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/EMR_SetUp_KeyPair.html"},"Create a Key Pair"),". Make sure that you setup the EC2 key pair inside the region in which you will be running your ETL jobs."),(0,o.kt)("h4",{id:"s3-locations"},"S3 locations"),(0,o.kt)("p",null,"EmrEtlRunner processes data through two states:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},":enriched"),"\xa0- Enriched Snowplow events are the input to the EmrEtlRunner process"),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},":shredded"),"\xa0- EmrEtlRunner shreds JSONs found in enriched events ready for loading into dedicated Redshift tables")),(0,o.kt)("p",null,"For\xa0",(0,o.kt)("inlineCode",{parentName:"p"},":enriched:stream"),", specify the Amazon S3 path you configured for your S3 loader."),(0,o.kt)("p",null,"For all other S3 locations, you can specify paths within a single S3 bucket that you setup now."),(0,o.kt)("p",null,"Done? Right, now we can install EmrEtlRunner."),(0,o.kt)("h3",{id:"installation"},"Installation"),(0,o.kt)("p",null,"EmrEtlRunner is hosted in the Releases section of the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/emr-etl-runner/releases"},"Github repo"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ wget https://github.com/snowplow/emr-etl-runner/releases/download/1.0.4/snowplow_emr_1.0.4.zip\n")),(0,o.kt)("p",null,"The archive contains both EmrEtlRunner and\xa0StorageLoader. Unzip the archive:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ unzip snowplow_emr_{{RELEASE_VERSION}}.zip\n")),(0,o.kt)("p",null,"The archive should contain a\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"snowplow-emr-etl-runner"),"\xa0file."),(0,o.kt)("h3",{id:"configuration"},"Configuration"),(0,o.kt)("p",null,"EmrEtlRunner requires a YAML format configuration file to run. There is a configuration file template available in the Snowplow GitHub repository at\xa0",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/emr-etl-runner/blob/master/config/config.yml.sample"},(0,o.kt)("inlineCode",{parentName:"a"},"https://github.com/snowplow/emr-etl-runner/blob/master/config/config.yml.sample")),". See\xa0",(0,o.kt)("a",{parentName:"p",href:"/docs/migrated/pipeline-components-and-applications/enrichment-components/emr-etl-runner/common-configuration"},"Common configuration"),"\xa0more information on how to write this file."),(0,o.kt)("h4",{id:"storage-targets"},(0,o.kt)("a",{parentName:"h4",href:"https://github.com/snowplow/snowplow/wiki/1-Installing-EmrEtlRunner#storage-targets"}),"Storage targets"),(0,o.kt)("p",null,"Storages for data can be configured using storage targets JSONs. Configuration file templates available in the Snowplow GitHub repository at\xa0",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/snowplow/blob/master/4-storage/config/targets/"},(0,o.kt)("inlineCode",{parentName:"a"},"/4-storage/config/targets"),"\xa0directory")),(0,o.kt)("h4",{id:"iglu"},(0,o.kt)("a",{parentName:"h4",href:"https://github.com/snowplow/snowplow/wiki/1-Installing-EmrEtlRunner#iglu"}),"Iglu"),(0,o.kt)("p",null,"You will also need an Iglu resolver configuration file. This is where we list the schema repositories to use to retrieve JSON Schemas for validation. For more information on this, see the\xa0",(0,o.kt)("a",{parentName:"p",href:"#shredding"},"wiki page for Configuring shredding"),"."),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"using-emretlrunner"},"Using EmrEtlRunner"),(0,o.kt)("p",null,"EmrEtlRunner works in\xa0",(0,o.kt)("strong",{parentName:"p"},"Rolling mode"),"\xa0where it processes whatever raw Snowplow event logs it finds in the In Bucket"),(0,o.kt)("h3",{id:"commands"},"Commands"),(0,o.kt)("h4",{id:"run-command"},"Run command"),(0,o.kt)("p",null,"The most useful command is the\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"run"),"\xa0command which allows you to actually run your EMR job:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ ./snowplow-emr-etl-runner run\n")),(0,o.kt)("p",null,"The available options are as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Usage: run [options]\n    -c, --config CONFIG              configuration file\n    -n, --enrichments ENRICHMENTS    enrichments directory\n    -r, --resolver RESOLVER          Iglu resolver file\n    -t, --targets TARGETS            targets directory\n    -d, --debug                      enable EMR Job Flow debugging\n    -f {enrich,shred,elasticsearch,archive_raw,rdb_load,analyze,archive_enriched,archive_shredded,staging_stream_enrich},\n        --resume-from                resume from the specified step\n    -x {staging,enrich,shred,elasticsearch,archive_raw,rdb_load,consistency_check,analyze,load_manifest_check,archive_enriched,archive_shredded,staging_stream_enrich},\n        --skip                       skip the specified step(s)\n    -i, --include {vacuum}           include additional step(s)\n    -l, --lock PATH                  where to store the lock\n        --ignore-lock-on-start       ignore the lock if it is set when starting\n        --consul ADDRESS             address to the Consul server\n")),(0,o.kt)("p",null,"Note that the\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"config"),"\xa0and\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"resolver"),"\xa0options are mandatory."),(0,o.kt)("p",null,"Note that in\xa0",(0,o.kt)("em",{parentName:"p"},"Stream Enrich mode"),"\xa0you cannot skip nor resume from\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"staging"),",\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"enrich"),"\xa0and\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"archive_raw"),". Instead of\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"staging"),"\xa0and\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"enrich"),", in Stream Enrich mode single special\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"staging_stream_enrich"),"\xa0is used."),(0,o.kt)("h4",{id:"lint-commands"},"Lint commands"),(0,o.kt)("p",null,"Other useful commands include the\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"lint"),"\xa0commands which allows you to check the validity of your resolver or enrichments with respect to their respective schemas."),(0,o.kt)("p",null,"If you want to lint your resolver:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ ./snowplow-emr-etl-runner lint resolver\n")),(0,o.kt)("p",null,"The mandatory options are:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Usage: lint resolver [options]\n    -r, --resolver RESOLVER          Iglu resolver file\n")),(0,o.kt)("p",null,"If you want to lint your enrichments:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ ./snowplow-emr-etl-runner lint enrichments\n")),(0,o.kt)("p",null,"The mandatory options are:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Usage: lint enrichments [options]\n    -r, --resolver RESOLVER          Iglu resolver file\n    -n, --enrichments ENRICHMENTS    enrichments directory\n")),(0,o.kt)("h3",{id:"checking-the-results"},"Checking the results"),(0,o.kt)("p",null,"Once you have run the EmrEtlRunner you should be able to manually inspect in S3 the folder specified in the\xa0",(0,o.kt)("inlineCode",{parentName:"p"},":out:"),"\xa0parameter in your\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"config.yml"),"\xa0file and see new files generated, which will contain the cleaned data either for uploading into a storage target (e.g. Redshift) or for analysing directly using Hive or Spark or some other querying tool on EMR."),(0,o.kt)("p",null,"Note: most Snowplow users run the 'spark' version of the ETL process, in which case the data generated is saved into subfolders with names of the form\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"part-000..."),". If, however, you are running the legacy 'hive' ETL (because e.g. you want to use Hive as your storage target, rather than Redshift, which is the only storage target the 'spark' etl currently supports), the subfolders names will be of the format\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"dt=..."),"."),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"scheduling-emretlrunner"},"Scheduling EmrEtlRunner"),(0,o.kt)("p",null,"Once you have the ETL process working smoothly, you can schedule a daily (or more frequent) task to automate the daily ETL process."),(0,o.kt)("p",null,"We run our daily ETL jobs at 3 AM UTC so that we are sure that we have processed all of the events from the day before (CloudFront logs can take some time to arrive)."),(0,o.kt)("p",null,"To consider your different scheduling options in turn:"),(0,o.kt)("h3",{id:"cron"},"cron"),(0,o.kt)("p",null,"Warning"),(0,o.kt)("p",null,"Running EmrEtlRunner as Ruby (rather than JRuby apps) is no longer actively supported."),(0,o.kt)("p",null,"The recommended way of scheduling the ETL process is as a daily cronjob."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"0 4   * * *   root    cronic /path/to/eer/snowplow-emr-etl-runner run -c config.yml\n")),(0,o.kt)("p",null,"This will run the ETL job daily at 4 AM, emailing any failures to you via cronic."),(0,o.kt)("h3",{id:"jenkins"},"Jenkins"),(0,o.kt)("p",null,"Some developers use the\xa0",(0,o.kt)("a",{parentName:"p",href:"http://jenkins-ci.org/"},"Jenkins"),"\xa0continuous integration server (or\xa0",(0,o.kt)("a",{parentName:"p",href:"http://hudson-ci.org/"},"Hudson"),", which is very similar) to schedule their Hadoop and Hive jobs."),(0,o.kt)("p",null,"Describing how to do this is out of scope for this guide, but the blog post\xa0",(0,o.kt)("a",{parentName:"p",href:"http://blog.lusis.org/blog/2012/01/23/lowtech-monitoring-with-jenkins/"},"Lowtech Monitoring with Jenkins"),"\xa0is a great tutorial on using Jenkins for non-CI-related tasks, and could be easily adapted to schedule EmrEtlRunner."),(0,o.kt)("h3",{id:"windows-task-scheduler"},"Windows Task Scheduler"),(0,o.kt)("p",null,"For Windows servers, in theory it should be possible to use a Windows PowerShell script plus\xa0",(0,o.kt)("a",{parentName:"p",href:"http://en.wikipedia.org/wiki/Windows_Task_Scheduler#Task_Scheduler_2.0"},"Windows Task Scheduler"),"\xa0instead of bash and cron. However, this has not been tested or documented."),(0,o.kt)("p",null,"If you get this working, please let us know!"),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"configuring-shredding"},"Configuring shredding"),(0,o.kt)("p",null,"Snowplow has a\xa0",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/snowplow/wiki/Shredding"},"Shredding process"),"\xa0for Redshift which contributes to the following three phases:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Extracting unstructured event JSONs and context JSONs from enriched event files into their own files"),(0,o.kt)("li",{parentName:"ol"},"Removing endogenous duplicate records, which are sometimes introduced within the Snowplow pipeline (feature added to r76)"),(0,o.kt)("li",{parentName:"ol"},"Loading those files into corresponding tables in Redshift")),(0,o.kt)("p",null,"The first two phases are instrumented by EmrEtlRunner; in this page we will explain how to configure the shredding process to operate smoothly with EmrEtlRunner."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note: Even though the first phase is required only if you want to shred your own unstructured event JSONs and context JSONs, the second phase will be beneficial to data modeling and analysis. If none of it is required and you are only shredding Snowplow-authored JSONs like link clicks and ad impressions, then you can skip this page and go straight to\xa0",(0,o.kt)("a",{parentName:"strong",href:"https://github.com/snowplow/snowplow/wiki/4-Loading-shredded-types"},"Loading shredded types"),".")),(0,o.kt)("h3",{id:"pre-requisites"},"Pre-requisites"),(0,o.kt)("p",null,"This guide assumes that"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"all JSONs you are sending as unstructured events and contexts are self-describing JSONs")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"you have defined self-describing JSON Schemas for each of your JSONs")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"you have setup your own Iglu schema registry to host your schemas"))),(0,o.kt)("h3",{id:"configuring-emretlrunner-for-shredding"},"Configuring EmrEtlRunner for shredding"),(0,o.kt)("p",null,"The relevant section of the EmrEtlRunner's\xa0",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample"},(0,o.kt)("inlineCode",{parentName:"a"},"config.yml")),"\xa0is:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"shredded:\n  good: s3://my-out-bucket/shredded/good       # e.g. s3://my-out-bucket/shredded/good\n  bad: s3://my-out-bucket/shredded/bad        # e.g. s3://my-out-bucket/shredded/bad\n  errors: s3://my-out-bucket/shredded/errors     # Leave blank unless :continue_on_unexpected_error: set to true below\n  archive: s3://my-out-bucket/shredded/archive  # Not required for Postgres currently\n")),(0,o.kt)("p",null,"The configuration file is referenced with\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"--config"),"\xa0option to EmrEtlRunner."),(0,o.kt)("p",null,"Please make sure that these shredded buckets are set correctly."),(0,o.kt)("p",null,"Next, we let EmrEtlRunner know about your Iglu schema registry, so that schemas can be retrieved from there as well as from Iglu Central. Add your own registry to the repositories array in\xa0",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/snowplow/blob/master/3-enrich/config/iglu_resolver.json"},(0,o.kt)("inlineCode",{parentName:"a"},"iglu_resolver.json")),"\xa0file:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'{\n  "schema": "iglu:com.snowplowanalytics.iglu/resolver-config/jsonschema/1-0-0",\n  "data": {\n    "cacheSize": 500,\n    "repositories": [\n      {\n        "name": "Iglu Central",\n        "priority": 0,\n        "vendorPrefixes": [ "com.snowplowanalytics" ],\n        "connection": {\n          "http": {\n            "uri": "http://iglucentral.com"\n          }\n        }\n      }\n      #custom section starts here --\x3e\n      ,\n      {\n       ...\n      }\n      #custom section ends here <--\n    ]\n  }\n}\n')),(0,o.kt)("p",null,"You must add an extra entr(-y/ies) in the\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"repositories:"),"\xa0array pointing to your own Iglu schema registry. If you are not submitting custom events and contexts and are not interested in shredding then there's no need in adding the custom section but the\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"iglu_resolver.json"),"\xa0file is still required and is referenced with\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"--resolver"),"\xa0option to EmrEtlRunner."),(0,o.kt)("p",null,"For more information on how to customize the\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"iglu_resolver.json"),"\xa0file, please review the\xa0",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/iglu/wiki/Iglu-client-configuration"},"Iglu client configuration"),"\xa0wiki page."),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"technical-explanation"},"Technical explanation"),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(92721).Z,width:"960",height:"720"})),(0,o.kt)("p",null,"Raw collector logs that need to be processed are identified in the in-bucket. (This is the bucket that the collector log files are generated in: it's location is specified in the\xa0",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample"},"EmrEtlRunner config file"),".)"),(0,o.kt)("p",null,"EmrEtlRunner then triggers the Enrichment process to run. It spins up an EMR cluster (the size of which is determined by the\xa0",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample"},"config file"),"), uploads the JAR with the Spark Enrichment process on, and instructs EMR to:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Use S3DistCopy to aggregate the collector log files and write them to HDFS"),(0,o.kt)("li",{parentName:"ol"},"Run the Enrichment process on those aggregated files in HDFS"),(0,o.kt)("li",{parentName:"ol"},"Write the output of that Enrichment to the Out-bucket in S3. (As specified in the\xa0",(0,o.kt)("a",{parentName:"li",href:"https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample"},"config file"),")."),(0,o.kt)("li",{parentName:"ol"},"When the job has completed, EmrEtlRunner moves the processed collector log files from the in-bucket to the archive bucket. (This, again, is specified in the\xa0",(0,o.kt)("a",{parentName:"li",href:"https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample"},"config file"),".)")),(0,o.kt)("p",null,"By setting up a\xa0",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/snowplow/wiki/3-Scheduling-EmrEtlRunner"},"scheduling job"),"\xa0to run EmrEtlRunner regularly, Snowplow users can ensure that the event data regularly flows through the Snowplow data pipeline from the collector to storage."),(0,o.kt)("p",null,"Note: many references are made to the 'Hadoop ETL' and 'Hive ETL' in the documentation and the\xa0",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample"},"config file"),". 'Hadoop ETL' refers to the current Spark-based Enrichment Process. 'Hive ETL' refers to the legacy Hive-based ETL process. EmrEtlRunner can be setup to instrument either. However, we recommend\xa0",(0,o.kt)("strong",{parentName:"p"},"all"),"\xa0Snowplow users use the Spark based 'Hadoop ETL', as it is much more robust, as well as being cheaper to run."),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"setting-up-end-to-end-encryption"},"Setting up end-to-end encryption"),(0,o.kt)("p",null,"It is possible to setup end-to-end encryption for EmrEtlRunner. For reference, you can check out the dedicated EMR guide:\xa0",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html"},"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html"),"."),(0,o.kt)("h3",{id:"pre-requisites-1"},"Pre-requisites"),(0,o.kt)("h4",{id:"encrypting-s3-buckets"},"Encrypting S3 buckets"),(0,o.kt)("p",null,"For at rest encryption on S3, the buckets with which EmrEtlRunner will interact need to have SSE-S3 encryption enabled."),(0,o.kt)("p",null,"For more information, check out the dedicated AWS guide:\xa0",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"},"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"),"."),(0,o.kt)("p",null,"Keep in mind that turning this setting on is not retroactive. It effectively means that if you want to have only encrypted data in your bucket you will need to go through the existing data and copy it in place."),(0,o.kt)("p",null,"Also, if you're using the Clojure Collector, SSE-S3 encryption needs to be set up at the bucket level."),(0,o.kt)("h4",{id:"setting-up-an-emr-security-configuration"},"Setting up an EMR security configuration"),(0,o.kt)("p",null,"Through an EMR security configuraton, you can specify at the EMR level for which parts of your job you want encryption to be enforced, the possibilities are:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"At rest on S3"),(0,o.kt)("li",{parentName:"ul"},"At rest on local disks"),(0,o.kt)("li",{parentName:"ul"},"In-transit")),(0,o.kt)("h4",{id:"at-rest-encryption-in-s3"},"At rest encryption in S3"),(0,o.kt)("p",null,"Once setup, S3 encrypts data as it writes it to disk."),(0,o.kt)("p",null,"By default, even without encryption setup, data is encrypted while in transit from EMR to S3 (e.g. for s3-dist-cp steps)."),(0,o.kt)("h4",{id:"at-rest-encryption-on-local-disks"},"At rest encryption on local disks"),(0,o.kt)("p",null,"When running the Snowplow pipeline in EMR, an HDFS is setup on the different nodes of your cluster. Enabling encryption for the disks local to those nodes will have the following effects:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"HDFS RPC, e.g. between name node and data node, uses SASL"),(0,o.kt)("li",{parentName:"ul"},"HDFS block transfers (e.g. replication) are encrypted using AES 256"),(0,o.kt)("li",{parentName:"ul"},"Attached EBS volumes are encrypted using\xa0",(0,o.kt)("a",{parentName:"li",href:"https://guardianproject.info/code/luks/"},"LUKS"))),(0,o.kt)("p",null,"When enabling this option, please keep the following drawbacks in mind:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"EBS root volumes are not encrypted, you need to use a custom AMI for that:\xa0",(0,o.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html"},"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html")),(0,o.kt)("li",{parentName:"ul"},"KMS key usage is subject to pricing:\xa0",(0,o.kt)("a",{parentName:"li",href:"https://aws.amazon.com/kms/pricing/"},"https://aws.amazon.com/kms/pricing/")),(0,o.kt)("li",{parentName:"ul"},"It has a performance impact (e.g. when writing your enriched data to HDFS)")),(0,o.kt)("p",null,"To set this type of encryption up you will need to create an appropriate KMS key, refer to the AWS guide for more information:\xa0",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html"},"https://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html"),"."),(0,o.kt)("p",null,"It is important to note that the role used in\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"aws:emr:jobflow_role"),"\xa0in the EmrEtlRunner configuration needs to have the\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"kms:GenerateDataKey"),"\xa0policy."),(0,o.kt)("h4",{id:"in-transit-encryption-spark-and-mapreduce"},"In-transit encryption (Spark and MapReduce)"),(0,o.kt)("p",null,"When running the Spark jobs of the Snowplow pipeline (enrich and shred) or consolidating s3-dist-cp steps (e.g. using\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"--groupBy"),"\xa0or\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"--targetSize"),"), data is shuffled around the different nodes in your EMR cluster. Enabling encryption for those data movements will have the following effects:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"MapReduce shuffles use TLS"),(0,o.kt)("li",{parentName:"ul"},"RPC and data transfers in Spark are encrypted using AES 256 if emr >= 5.9.0, otherwise RPC is encrypted using SASL"),(0,o.kt)("li",{parentName:"ul"},"SSL is enabled for all things HTTP in Spark (e.g. history server and UI)")),(0,o.kt)("p",null,"Be aware that this type of encryption also has a performance impact as data needs to be encrypted when sent over the network (e.g. when running deduplication in the Shred job)."),(0,o.kt)("p",null,"To set up this type of encryption, you will need to create certificates according to the guidelines specified at\xa0",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-encryption-enable.html#emr-encryption-pem-certificate"},"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-encryption-enable.html#emr-encryption-pem-certificate"),"."),(0,o.kt)("p",null,"Note that, for this type of encryption to work, you will need to be in a VPC and the domain name specified in the certificates needs to be\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"*.ec2.internal"),"\xa0if in us-east-1 or\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"*.region.compute.internal"),"\xa0otherwise."),(0,o.kt)("p",null,"For more information, on all those types of encryption, you can refer to the dedicated guide:\xa0",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html"},"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html"),"."),(0,o.kt)("h3",{id:"configuring-emretlrunner-for-end-to-end-encryption"},"Configuring EmrEtlRunner for end-to-end encryption"),(0,o.kt)("p",null,"To leverage the security configuration you created, you will need to specify it in the EmrEtlRunner configuration at:\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"aws:emr:security_configuration"),"."),(0,o.kt)("p",null,"Additionally, you will need to tell EmrEtlRunner that it will have to interact with encrypted buckets through:\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"aws:s3:buckets:encrypted: true"),"."),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"setting-up-ec2-instance-for-emretlrunner-and-storageloader"},"Setting up EC2 instance for EmrEtlRunner and StorageLoader"),(0,o.kt)("p",null,"This tutorial assumes it's your first installation and you probably just want to checkout the platform. Thus many steps describe low-performance and unsecured installation. In real-world scenario you may want to fix that."),(0,o.kt)("h3",{id:"prepare-your-system"},(0,o.kt)("a",{parentName:"h3",href:"https://github.com/snowplow/snowplow/wiki/Setting-up-EC2-instance-for-EmrEtlRunner-and-StorageLoader#prepare-your-system"}),"Prepare your system"),(0,o.kt)("p",null,"Before getting started you need to have:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Account on\xa0",(0,o.kt)("a",{parentName:"li",href:"http://aws.amazon.com/"},"Amazon Web Services"),"."),(0,o.kt)("li",{parentName:"ul"},"Installed\xa0",(0,o.kt)("a",{parentName:"li",href:"https://aws.amazon.com/cli/"},"AWS CLI"),"."),(0,o.kt)("li",{parentName:"ul"},"IAM user, first one need to be created in\xa0",(0,o.kt)("a",{parentName:"li",href:"https://console.aws.amazon.com/iam/home?#users"},"AWS Console"),"."),(0,o.kt)("li",{parentName:"ul"},"IAM user need to have attached\xa0",(0,o.kt)("inlineCode",{parentName:"li"},"AdministratorAccess"),"."),(0,o.kt)("li",{parentName:"ul"},"Configured credentials on your local machine. (You can use\xa0",(0,o.kt)("inlineCode",{parentName:"li"},"aws configure"),"\xa0for it)."),(0,o.kt)("li",{parentName:"ul"},"For some steps you may want to install\xa0",(0,o.kt)("a",{parentName:"li",href:"https://stedolan.github.io/jq/"},(0,o.kt)("inlineCode",{parentName:"a"},"jq")),". It's optional, but handy.")),(0,o.kt)("p",null,"Everything else can be done from CLI."),(0,o.kt)("h3",{id:"setting-up-ec2-instance-for-emretlrunnerstorageloader"},(0,o.kt)("a",{parentName:"h3",href:"https://github.com/snowplow/snowplow/wiki/Setting-up-EC2-instance-for-EmrEtlRunner-and-StorageLoader#setting-up-ec2-instance-for-emretlrunnerstorageloader"}),"Setting up EC2 instance for EmrEtlRunner/StorageLoader"),(0,o.kt)("p",null,"In the end of this step, you'll have an AWS EC2 instance, SSH access to it and key stored on local machine."),(0,o.kt)("h3",{id:"find-your-default-vpc-id"},(0,o.kt)("strong",{parentName:"h3"},"Find your Default VPC ID")),(0,o.kt)("p",null,"We will refer to it as\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"{{ VPC_ID }}"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'$ aws ec2 describe-vpcs | jq -r ".Vpcs[0].VpcId"\n')),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"NOTE"),": This step assumes the default VPC will be first in the list. If your environment has multiple VPCs, run the describe command without piping it to\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"jq"),"\xa0to get the ID of the default VPC. Mixing VPC IDs will result in the creation of an unreachable EC2 instance."),(0,o.kt)("h3",{id:"create-security-group-for-ssh-access"},(0,o.kt)("strong",{parentName:"h3"},"Create Security Group for SSH access")),(0,o.kt)("p",null,"On output you'll get\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"GroupId"),". We will refer to it as\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"{{ SSH_SG }}"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'$ aws ec2 create-security-group \\\n    --group-name "EC2 SSH full access" \\\n    --description "Unsafe. Use for demonstration only" \\\n    --vpc-id {{ VPC_ID }} \\\n    | jq -r \'.GroupId\'\n')),(0,o.kt)("h3",{id:"add-rule-allowing-ssh-access-from-anywhere"},(0,o.kt)("strong",{parentName:"h3"},"Add rule allowing SSH access from anywhere")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ aws ec2 authorize-security-group-ingress \\\n    --group-id {{ SSH_SG }} \\\n    --protocol tcp \\\n    --port 22 \\\n    --cidr 0.0.0.0/0\n")),(0,o.kt)("h3",{id:"create-ssh-key-pair-named-on-the-local-machine"},(0,o.kt)("strong",{parentName:"h3"},"Create SSH key-pair named on the local machine")),(0,o.kt)("p",null,'We named it "snowplow-ec2" here.'),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'$ aws ec2 create-key-pair --key-name snowplow-ec2 \\\n    | jq -r ".KeyMaterial" &gt; ~/.ssh/snowplow-ec2.pem\n$ chmod go-rwx ~/.ssh/snowplow-ec2.pem\n')),(0,o.kt)("h3",{id:"run-t2small-instance-with-amazon-linux-ami-with-previously-created-ssh-key"},(0,o.kt)("strong",{parentName:"h3"},"Run t2.small instance with Amazon Linux AMI with previously created SSH-key")),(0,o.kt)("p",null,"On output you will get your instance id. We will refer to it as\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"{{ INSTANCE_ID }}"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ aws ec2 run-instances \\\n    --image-id ami-60b6c60a \\\n    --count 1 \\\n    --instance-type t2.small \\\n    --key-name snowplow-ec2 \\\n    | jq -r '.Instances[0].InstanceId'\n")),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"NOTE: you can find available image ID by following this\xa0",(0,o.kt)("a",{parentName:"em",href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/finding-an-ami.html"},"AWS guide"),".")),(0,o.kt)("h3",{id:"attach-security-group-to-instance"},(0,o.kt)("strong",{parentName:"h3"},"Attach security group to Instance")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ aws ec2 modify-instance-attribute \\\n    --instance-id {{ INSTANCE_ID }} \\\n    --groups {{ SSH_SG }}\n")),(0,o.kt)("h3",{id:"check-public-ip-address-of-newly-created-instance"},(0,o.kt)("strong",{parentName:"h3"},"Check public IP-address of newly created Instance")),(0,o.kt)("p",null,"Further we will refer to it as\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"{{ PUBLIC_IP }}"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ aws ec2 describe-instances \\\n    --instance-ids {{ INSTANCE_ID }} \\\n    | jq '.Reservations[0].Instances[0].PublicDnsName'\n")),(0,o.kt)("h3",{id:"log-in"},(0,o.kt)("strong",{parentName:"h3"},"Log-in")),(0,o.kt)("p",null,"Fill-in\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"{{ PUBLIC_IP }}"),"\xa0from previous step."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ ssh -i ~/.ssh/snowplow-ec2.pem ec2-user@{{ PUBLIC_IP }}\n")))}c.isMDXComponent=!0},92721:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/emr-etl-runner-steps-f0188e637f63d72b6102a4ec980ca37b.png"}}]);